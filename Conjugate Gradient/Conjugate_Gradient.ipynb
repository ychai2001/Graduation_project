{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911e8758",
   "metadata": {
    "id": "911e8758"
   },
   "source": [
    "# Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-definition",
   "metadata": {
    "id": "alleged-definition"
   },
   "source": [
    "## 경사하강법 (Steepest Decent method)\n",
    "\n",
    "$A$ 행렬이 Positive Definite인 경우 $Ax=b$ 문제는 다음 에너지 $\\phi$ 를 최소화 하는 문제와 같다.\n",
    "\n",
    "$$\n",
    "\\min_x \\phi(x) = \\frac{1}{2} x^T A x - x^T b\n",
    "$$\n",
    "\n",
    "잔차 (Residual) $r(x)$ 은 다음과 같이 정의한다.\n",
    "\n",
    "$$\n",
    "r(x) = -\\nabla \\phi(x) = b - Ax\n",
    "$$\n",
    "\n",
    "현 위치에서 가장 변화율이 심한 방향은 $r(x)=-\\nabla \\phi(x)$ 이다.\n",
    "경사 하강법은 이 방향으로 해를 찾는 방법이다. 즉\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + \\alpha r_{k} \\\\\n",
    "r_{k} = r(x_{k})\n",
    "$$\n",
    "\n",
    "여기서 $\\alpha$ 는 학습률이다. 에너지 식의 Variation이 0이 되는 학습률을 구하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\delta \\phi = \\phi (x_{k} + \\alpha r_{k})-\\phi (x_{k}) \\\\\n",
    "\\frac{d\\Delta \\phi}{d\\alpha} = r_k^T A x_k + \\alpha r_k^T A r_k - r_k^T b = r_k^T r_k - \\alpha r_k^T A r_k=0\n",
    "$$\n",
    "\n",
    "즉\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{r_k^T r_k}{r_k^T A r_k}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "round-yukon",
   "metadata": {
    "id": "round-yukon"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from iterative import cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe561cae",
   "metadata": {
    "id": "fe561cae"
   },
   "outputs": [],
   "source": [
    "def sd(A, b, x0=None, tol=1e-12, itmax=5000, verbose=False):\n",
    "    \"\"\"\n",
    "    Steepest Decent method\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : array\n",
    "        Operation matrix\n",
    "    b : array\n",
    "        Forcing term\n",
    "    tol : float\n",
    "        tolerance\n",
    "    itmax : integer\n",
    "        maximum iteration\n",
    "    verbose : bool\n",
    "        print intermediate solution\n",
    "    \n",
    "    Result\n",
    "    ------\n",
    "    x : array\n",
    "        solution\n",
    "    \"\"\"\n",
    "    if not x0:\n",
    "        # Initial guess\n",
    "        x = b.copy()\n",
    "    else:\n",
    "        x = x0\n",
    "\n",
    "    for n in range(itmax):\n",
    "        # Compute Residual\n",
    "        r = b - A @ x\n",
    "        r2 = r.T @ r\n",
    "\n",
    "        if np.sqrt(r2) < tol:\n",
    "            break\n",
    "        else:\n",
    "            # Compute Learning rate\n",
    "            alpha =  r2 / (r.T @ A @ r)\n",
    "\n",
    "            # Update learning rate\n",
    "            x += alpha*r\n",
    "            \n",
    "        if verbose:\n",
    "            print(n, x)\n",
    "        \n",
    "    return x, n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-mason",
   "metadata": {
    "id": "hispanic-mason"
   },
   "source": [
    "## Conjugate Gradient Method\n",
    "해를 찾는 방향 벡터가 $r_k$ 가 아니고 $d_k$ 라 하자.\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + \\alpha d_k\n",
    "$$\n",
    "\n",
    "이 경우에도 $\\delta \\phi=0$ 을 만족시키는 학습률은 아래와 같다.\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{r_k^T d_k}{d_k^T A d_k}.\n",
    "$$\n",
    "\n",
    "Conjugate Gradient method는 방향 벡터 $d_k$ 가 $A$ Conjugate (또는 $A$-Orthognal)을 만족한다.\n",
    "\n",
    "$$\n",
    "d_i^T A d_j = 0~~~\\textrm{for all}~i \\neq j.\n",
    "$$\n",
    "\n",
    "이 방향벡터 $d_k$ 는 residual $r_k$ 의 선형 결합으로 구성한다.\n",
    "\n",
    "$$\n",
    "d_k = r_k + \\beta_k d_{k-1}\n",
    "$$\n",
    "\n",
    "$A$ Conjugate 를 만족하기 위해서는 위 식에서 양변에 $d_{k-1}^T A$를 곱한다.\n",
    "\n",
    "$$\n",
    "d_{k-1}^T A d_k = d_{k-1}^T A (r_k + \\beta_k d_{k-1}).\n",
    "$$\n",
    "\n",
    "$d_{k-1}^T A d_k=0$ 이므로 $\\beta_k$ 는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\beta_k = -\\frac{d_{k-1}^T A r_k}{d_{k-1}^T A d_{k-1}}\n",
    "$$\n",
    "\n",
    "다음 성질을 만족한다.\n",
    "\n",
    "### Theorem 1 (Orthogonality of residual)\n",
    "$$\n",
    "r_{k+1}^T r_i = 0~~\\textrm{for all}~i \\le k \n",
    "$$\n",
    "\n",
    "#### Proof\n",
    "$x_k$ 는 $x_0$ 와 $d_1, d_2,...,d_k$ 의 선형 결합으로 표현할 수 있다. \n",
    "\n",
    "$$\n",
    "x_{k+1} = x_0 + D_k y\n",
    "$$\n",
    "\n",
    "$D_k$ 를 이들 방향 벡터로 구성한 Matrix이다. 이를 에너지 식에 적용하면\n",
    "\n",
    "$$\n",
    "\\phi(x_0 + D_k y) = \\phi(x_0) + \\frac{1}{2} y^T (D_k^T A D_k) y - y^T D_k^T(b - Ax_0)\n",
    "$$\n",
    "\n",
    "에너지가 최소화 되어야 하므로\n",
    "\n",
    "$$\n",
    "(D_k^T A D_k) y = D_k^T (b - Ax_0).\n",
    "$$\n",
    "\n",
    "즉\n",
    "\n",
    "$$\n",
    "D_k^T (b- Ax_0) - (D_k^T A D_k) y = D^k (b - A(x_0 + D_ky)) = D_k ^T r_{k+1} = 0\n",
    "$$\n",
    "\n",
    "그러므로 \n",
    "\n",
    "$$\n",
    "r_{k+1}^T d_i = 0~~\\textrm{for all}~i \\le k\n",
    "$$\n",
    "\n",
    "이를 이용하면 $i \\le k$ 인 경우\n",
    "\n",
    "$$\n",
    "r_{k+1}^T r_i = r_{k+1}^T (d_i - \\beta_i d_{i-1})=0\n",
    "$$\n",
    "\n",
    "즉 Residual은 서로 수직임을 알 수 있다.\n",
    "\n",
    "\n",
    "### 구현\n",
    "Residual의 직교성을 이용하면\n",
    "$$\n",
    "d_k - r_k = \\beta_k d_{k-1} \\\\\n",
    "$$\n",
    "\n",
    "이므로\n",
    "\n",
    "$$\n",
    "r_k^T(d_k - r_k) = \\beta_k r_k^T d_{k-1}=0\n",
    "$$\n",
    "\n",
    "그러므로\n",
    "\n",
    "$$\n",
    "r_k^T d_k = r_k^T r_k\n",
    "$$\n",
    "\n",
    "즉\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{r_k^T d_k}{d_k^T A d_k} = \\frac{r_k^T r_k}{d_k^T A d_k}\n",
    "$$\n",
    "\n",
    "$r_k=b - Ax_k$ 이므로\n",
    "\n",
    "$$\n",
    "r_{k+1} - r_k = -A (x_{k+1} - x_k) = -\\alpha_k A d_k.\n",
    "$$\n",
    "\n",
    "이를 이용하면\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = -\\frac{d_{k}^T A r_{k+1}}{d_{k}^T A d_{k}} =  -\\frac{r_{k+1}^T(r_{k+1} - r_k)}{d_k^T(r_{k+1} - r_k)}\n",
    "$$\n",
    "\n",
    "여기서 $d_k^T r_{k+1}=0$, $r_k^T d_k= r_k^T r_k$ 이고\n",
    "\n",
    "$$\n",
    "r_{k+1}^T r_k= r_{k+1}^T (d_k + \\beta_k d_{k-1}) = r_{k+1}^T d_k + \\beta_k r_{k+1}^Td_{k-1} = 0\n",
    "$$\n",
    "\n",
    "을 적용하면\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} =  \\frac{r_{k+1}^Tr_{k+1}}{r_k^T r_k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-appreciation",
   "metadata": {
    "id": "lightweight-appreciation"
   },
   "source": [
    "$$\n",
    "r_k = b - Ax_{k}\\\\\n",
    "r_{k} - r_{k-1} = - A (x_{k} - x_{k-1}) = -\\alpha_{k-1} A d_{k-1} \\\\\n",
    "d_i^T (r_k - r_{k-1}) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35891fce",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\text{Compute} \\; \\; r_0 := b - Ax_0, \\; p_0 := r_0 \\\\\n",
    "& \\text{For} \\; j = 0, 1, \\cdots, \\; \\text{until convergence Do:} \\\\\n",
    "& \\qquad \\alpha_j \\,:=\\, (r_j, r_j) / (A_{p_j}, p_j) \\\\\n",
    "& \\qquad x_{j+1} \\,:=\\, x_j + \\alpha_j p_j \\\\\n",
    "& \\qquad r_{j+1} \\,:=\\, r_j - \\alpha_j A p_j \\\\\n",
    "& \\qquad \\beta_j \\,:=\\, (r_{j+1}, r_{j+1}) / (r_j, r_j) \\\\\n",
    "& \\qquad p_{j+1} \\,:=\\, r_{j+1} + \\beta_j p_j \\\\\n",
    "& \\text{EndDo} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exterior-giving",
   "metadata": {
    "id": "exterior-giving"
   },
   "outputs": [],
   "source": [
    "def CG(A, b, x0=None, tol=1e-12, itmax=5000, verbose=False):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient Method\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : array\n",
    "        Operation matrix\n",
    "    b : array\n",
    "        Forcing term\n",
    "    tol : float\n",
    "        tolerance\n",
    "    itmax : integer\n",
    "        maximum iteration\n",
    "    verbose : bool\n",
    "        print intermediate solution\n",
    "    \n",
    "    Result\n",
    "    ------\n",
    "    x : array\n",
    "        solution\n",
    "    \"\"\"\n",
    "    \n",
    "    if not x0:\n",
    "        # Initial guess\n",
    "        x = b.copy()\n",
    "    else:\n",
    "        x = x0\n",
    "        \n",
    "    r = b - A @ x\n",
    "    r2 = r @ r\n",
    "\n",
    "    for n in range(itmax):\n",
    "        if np.sqrt(r2) < tol:\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Direction vector\n",
    "            if n == 0:\n",
    "                d = r.copy()\n",
    "            else:\n",
    "                # Update direction\n",
    "                beta = r @ r / r20\n",
    "                d = r + beta*d\n",
    "                \n",
    "            Ad = A @ d\n",
    "\n",
    "            # Update x and residual\n",
    "            alpha = r2 / (d @ Ad)\n",
    "            x += alpha*d\n",
    "            r -= alpha*Ad\n",
    "            \n",
    "            # Update magnitude of residual\n",
    "            r20 = r2\n",
    "            r2 = r @ r\n",
    "            \n",
    "        if verbose:\n",
    "            print(n, x)\n",
    "            \n",
    "    return x, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-patrol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1685519784355,
     "user": {
      "displayName": "박진석/조교수/항공우주공학과",
      "userId": "07894960443296492819"
     },
     "user_tz": -540
    },
    "id": "better-patrol",
    "outputId": "bc2be64d-ea9c-48b8-8f4b-fb47fdb1c8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution by linear algebra:\n",
      " [1.         0.5        0.33333333 0.25      ]\n",
      "0 [0.4 0.4 0.4 0.4]\n",
      "1 [0.64 0.48 0.32 0.16]\n",
      "2 [0.784 0.496 0.336 0.304]\n",
      "3 [0.8704 0.4992 0.3328 0.2176]\n",
      "4 [0.92224 0.49984 0.33344 0.26944]\n",
      "5 [0.953344 0.499968 0.333312 0.238336]\n",
      "6 [0.9720064 0.4999936 0.3333376 0.2569984]\n",
      "7 [0.98320384 0.49999872 0.33333248 0.24580096]\n",
      "8 [0.9899223  0.49999974 0.3333335  0.25251942]\n",
      "9 [0.99395338 0.49999995 0.3333333  0.24848835]\n",
      "10 [0.99637203 0.49999999 0.33333334 0.25090699]\n",
      "11 [0.99782322 0.5        0.33333333 0.2494558 ]\n",
      "12 [0.99869393 0.5        0.33333333 0.25032652]\n",
      "13 [0.99921636 0.5        0.33333333 0.24980409]\n",
      "14 [0.99952982 0.5        0.33333333 0.25011755]\n",
      "15 [0.99971789 0.5        0.33333333 0.24992947]\n",
      "16 [0.99983073 0.5        0.33333333 0.25004232]\n",
      "17 [0.99989844 0.5        0.33333333 0.24997461]\n",
      "18 [0.99993906 0.5        0.33333333 0.25001523]\n",
      "19 [0.99996344 0.5        0.33333333 0.24999086]\n",
      "20 [0.99997806 0.5        0.33333333 0.25000548]\n",
      "21 [0.99998684 0.5        0.33333333 0.24999671]\n",
      "22 [0.9999921  0.5        0.33333333 0.25000197]\n",
      "23 [0.99999526 0.5        0.33333333 0.24999882]\n",
      "24 [0.99999716 0.5        0.33333333 0.25000071]\n",
      "25 [0.99999829 0.5        0.33333333 0.24999957]\n",
      "26 [0.99999898 0.5        0.33333333 0.25000026]\n",
      "27 [0.99999939 0.5        0.33333333 0.24999985]\n",
      "28 [0.99999963 0.5        0.33333333 0.25000009]\n",
      "29 [0.99999978 0.5        0.33333333 0.24999994]\n",
      "30 [0.99999987 0.5        0.33333333 0.25000003]\n",
      "31 [0.99999992 0.5        0.33333333 0.24999998]\n",
      "32 [0.99999995 0.5        0.33333333 0.25000001]\n",
      "33 [0.99999997 0.5        0.33333333 0.24999999]\n",
      "34 [0.99999998 0.5        0.33333333 0.25      ]\n",
      "35 [0.99999999 0.5        0.33333333 0.25      ]\n",
      "36 [0.99999999 0.5        0.33333333 0.25      ]\n",
      "37 [1.         0.5        0.33333333 0.25      ]\n",
      "38 [1.         0.5        0.33333333 0.25      ]\n",
      "39 [1.         0.5        0.33333333 0.25      ]\n",
      "40 [1.         0.5        0.33333333 0.25      ]\n",
      "41 [1.         0.5        0.33333333 0.25      ]\n",
      "42 [1.         0.5        0.33333333 0.25      ]\n",
      "43 [1.         0.5        0.33333333 0.25      ]\n",
      "44 [1.         0.5        0.33333333 0.25      ]\n",
      "45 [1.         0.5        0.33333333 0.25      ]\n",
      "46 [1.         0.5        0.33333333 0.25      ]\n",
      "47 [1.         0.5        0.33333333 0.25      ]\n",
      "48 [1.         0.5        0.33333333 0.25      ]\n",
      "49 [1.         0.5        0.33333333 0.25      ]\n",
      "50 [1.         0.5        0.33333333 0.25      ]\n",
      "51 [1.         0.5        0.33333333 0.25      ]\n",
      "52 [1.         0.5        0.33333333 0.25      ]\n",
      "53 [1.         0.5        0.33333333 0.25      ]\n",
      "54 [1.         0.5        0.33333333 0.25      ]\n",
      "Solution by steepest desent:\n",
      " [1.         0.5        0.33333333 0.25      ]\n",
      "0 [0.4 0.4 0.4 0.4]\n",
      "1 [0.8 0.6 0.4 0.2]\n",
      "2 [0.97142857 0.54285714 0.3047619  0.25714286]\n",
      "3 [1.         0.5        0.33333333 0.25      ]\n",
      "Solution by congugate gradient:\n",
      " [1.         0.5        0.33333333 0.25      ]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "A = np.diag([1,2,3,4])\n",
    "b = np.array([1,1,1,1])\n",
    "\n",
    "print('Solution by linear algebra:\\n', np.linalg.solve(A, b))\n",
    "print('Solution by steepest desent:\\n', sd(A, b, x0=[0,0,0,0], verbose=True)[0])\n",
    "print('Solution by congugate gradient:\\n', CG(A, b, x0=[0.,0.,0.,0.], verbose=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf8db8c",
   "metadata": {
    "id": "fdf8db8c"
   },
   "outputs": [],
   "source": [
    "# Finite Difference Matrix\n",
    "def fd(n):\n",
    "    A = np.zeros((n,n))\n",
    "\n",
    "    for i in range(n):\n",
    "        A[i,i] = 2\n",
    "        if i > 0:\n",
    "            A[i-1,i] = -1\n",
    "        if i < n-1:\n",
    "            A[i+1, i] = -1\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ca2e3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1685519784357,
     "user": {
      "displayName": "박진석/조교수/항공우주공학과",
      "userId": "07894960443296492819"
     },
     "user_tz": -540
    },
    "id": "b6ca2e3a",
    "outputId": "7d71a719-3a77-47dc-cb35-a30dc857c6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution by linear algebra:\n",
      " [ 15.15431454  29.36374014  42.62122868  55.25280163  67.21458479\n",
      "  79.07639157  90.90250383 102.54246644 113.84862757 124.79335583\n",
      " 135.43086714 145.48462786 154.84136192 164.1450708  173.36698693\n",
      " 181.7034383  189.37960837 196.57811388 203.19105377 209.63844525\n",
      " 215.8851804  221.54297319 226.78073457 231.8117255  236.37678117\n",
      " 240.31280328 243.62258003 246.22142417 248.12951031 249.70462761\n",
      " 250.53438422 250.75749076 250.27580978 249.26057532 247.94666146\n",
      " 245.96410861 243.71874504 240.68830459 237.49459851 234.2308276\n",
      " 230.21548003 226.10030088 221.58151169 216.2282854  210.62171087\n",
      " 204.96800676 198.94291435 192.57419462 185.78101781 178.38485005\n",
      " 170.76052177 162.33258369 153.34685846 143.65109475 133.95058574\n",
      " 124.06232196 113.23430355 101.57491911  89.24649673  76.1604495\n",
      "  62.24523227  47.56823487  32.32342712  16.41321849]\n",
      "Solution by steepest desent:\n",
      " (array([ 15.12963684,  29.3143975 ,  42.547426  ,  55.15457706,\n",
      "        67.09234621,  78.93020216,  90.73297065, 102.34967711,\n",
      "       113.63338267, 124.55576662, 135.17192021, 145.20445707,\n",
      "       154.54113068, 163.8249343 , 173.02827456, 181.34632513,\n",
      "       189.00557733, 196.18735831, 202.78519629, 209.21769565,\n",
      "       215.45128587, 221.09615799, 226.32285413, 231.3430165 ,\n",
      "       235.89918993, 239.82657668, 243.12973714, 245.72221974,\n",
      "       247.6260173 , 249.19710629, 250.02494208, 250.24639114,\n",
      "       249.76517503, 248.75066939, 247.43960176, 245.46015723,\n",
      "       243.21999467, 240.19501302, 237.00881417, 233.75280158,\n",
      "       229.74719734, 225.6420036 , 221.13510289, 215.79399585,\n",
      "       210.20134397, 204.5617798 , 198.55251419, 192.19982307,\n",
      "       185.42422945, 178.04582932, 170.44067646, 162.03207913,\n",
      "       153.0669425 , 143.39191208, 133.71321263, 123.84688111,\n",
      "       113.04168958, 101.40523158,  89.10044028,  76.03809963,\n",
      "        62.14709704,  47.494365  ,  32.27412936,  16.38851833]), 4999)\n",
      "Solution by congugate gradient in scipy:\n",
      " (array([ 15.15431454,  29.36374014,  42.62122868,  55.25280163,\n",
      "        67.21458479,  79.07639157,  90.90250383, 102.54246644,\n",
      "       113.84862757, 124.79335583, 135.43086714, 145.48462786,\n",
      "       154.84136192, 164.1450708 , 173.36698693, 181.7034383 ,\n",
      "       189.37960837, 196.57811388, 203.19105377, 209.63844525,\n",
      "       215.8851804 , 221.54297319, 226.78073457, 231.8117255 ,\n",
      "       236.37678117, 240.31280328, 243.62258003, 246.22142417,\n",
      "       248.12951031, 249.70462761, 250.53438422, 250.75749076,\n",
      "       250.27580978, 249.26057532, 247.94666146, 245.96410861,\n",
      "       243.71874504, 240.68830459, 237.49459851, 234.2308276 ,\n",
      "       230.21548003, 226.10030088, 221.58151169, 216.2282854 ,\n",
      "       210.62171087, 204.96800676, 198.94291435, 192.57419462,\n",
      "       185.78101781, 178.38485005, 170.76052177, 162.33258369,\n",
      "       153.34685846, 143.65109475, 133.95058574, 124.06232196,\n",
      "       113.23430355, 101.57491911,  89.24649673,  76.1604495 ,\n",
      "        62.24523227,  47.56823487,  32.32342712,  16.41321849]), 0)\n",
      "Solution by congugate gradient:\n",
      " (array([ 15.15431454,  29.36374014,  42.62122868,  55.25280163,\n",
      "        67.21458479,  79.07639157,  90.90250383, 102.54246644,\n",
      "       113.84862757, 124.79335583, 135.43086714, 145.48462786,\n",
      "       154.84136192, 164.1450708 , 173.36698693, 181.7034383 ,\n",
      "       189.37960837, 196.57811388, 203.19105377, 209.63844525,\n",
      "       215.8851804 , 221.54297319, 226.78073457, 231.8117255 ,\n",
      "       236.37678117, 240.31280328, 243.62258003, 246.22142417,\n",
      "       248.12951031, 249.70462761, 250.53438422, 250.75749076,\n",
      "       250.27580978, 249.26057532, 247.94666146, 245.96410861,\n",
      "       243.71874504, 240.68830459, 237.49459851, 234.2308276 ,\n",
      "       230.21548003, 226.10030088, 221.58151169, 216.2282854 ,\n",
      "       210.62171087, 204.96800676, 198.94291435, 192.57419462,\n",
      "       185.78101781, 178.38485005, 170.76052177, 162.33258369,\n",
      "       153.34685846, 143.65109475, 133.95058574, 124.06232196,\n",
      "       113.23430355, 101.57491911,  89.24649673,  76.1604495 ,\n",
      "        62.24523227,  47.56823487,  32.32342712,  16.41321849]), 64)\n"
     ]
    }
   ],
   "source": [
    "n = 64\n",
    "\n",
    "A = fd(n)\n",
    "b = np.random.rand(n)\n",
    "\n",
    "print('Solution by linear algebra:\\n', np.linalg.solve(A, b))\n",
    "print('Solution by steepest desent:\\n', sd(A, b))\n",
    "print('Solution by congugate gradient in scipy:\\n', cg(A, b))\n",
    "print('Solution by congugate gradient:\\n', CG(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6e3d8",
   "metadata": {},
   "source": [
    "## GPU 병렬화\n",
    "\n",
    "Conjugate Gradient는 크게 5가지의 연산으로 구성된다.\n",
    "\n",
    "- SpMV (Sparse matrix-vector product)\n",
    "- dot product\n",
    "- scalar-vector product\n",
    "- vector addition and subtraction\n",
    "\n",
    "이 연산들은 SIMD 패턴을 가지고 있으므로 GPU에서 계산하기에 적절하다.\n",
    "\n",
    "따라서 각 연산을 GPU에서 계산하는 커널을 작성하여 계산 시간을 단축할 수 있다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
